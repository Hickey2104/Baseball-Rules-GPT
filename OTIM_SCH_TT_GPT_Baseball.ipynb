{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hickey2104/Baseball-Rules-GPT/blob/main/OTIM_SCH_TT_GPT_Baseball.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBVyY22Mp5NI",
        "outputId": "d82c0ca7-17e2-489f-dfef-c9c39a82e535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Running on: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken torch\n",
        "\n",
        "import math, time, os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple GPT-style model\n",
        "\n",
        "class SimpleConfig:\n",
        "    def __init__(self, vocab_size, block_size=128, n_layer=6, n_head=8, n_embd=512):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.n_head = config.n_head\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        self.qkv = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                             .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.qkv(x).reshape(B, T, 3, self.n_head, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.proj(y)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class SimpleGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size\n",
        "        tok_emb = self.token_emb(idx)\n",
        "        x = tok_emb + self.pos_emb[:, :t, :]\n",
        "        x = self.drop(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        if targets is None:\n",
        "            return logits\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss"
      ],
      "metadata": {
        "id": "A3ES-Tc7p_GP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "text_path = '/content/drive/My Drive/cleaned_baseball_rules.txt' # Update this path to where your file is located in Google Drive\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")  # tokenizer\n",
        "vocab_size = enc.n_vocab\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, block_size):\n",
        "        text = Path(path).read_text(encoding=\"utf-8\")\n",
        "        self.ids = tokenizer.encode(text)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.ids[idx:idx+self.block_size], dtype=torch.long)\n",
        "        y = torch.tensor(self.ids[idx+1:idx+self.block_size+1], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "block_size = 128\n",
        "dataset = TextDataset(text_path, enc, block_size)\n",
        "loader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTsPr_AcqB_b",
        "outputId": "9b925a81-304b-4bf0-cc94-93486c2be81c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Vocab size: 50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = SimpleConfig(vocab_size, block_size)\n",
        "model = SimpleGPT(config).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.5) # learning rate scheduler\n",
        "\n",
        "max_steps = 50000\n",
        "gradient_accumulate_every = 8  # Accumulate gradients over this many batches\n",
        "\n",
        "print(\"Training...\")\n",
        "\n",
        "model.train()\n",
        "start = time.time()\n",
        "for step, (x, y) in enumerate(loader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    _, loss = model(x, y)\n",
        "    loss = loss / gradient_accumulate_every\n",
        "    loss.backward()\n",
        "\n",
        "    if (step + 1) % gradient_accumulate_every == 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step() # Step the scheduler\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    if step % 250 == 0:\n",
        "        print(f\"step {step:5d} | loss {loss.item():.4f} | lr {optimizer.param_groups[0]['lr']:.6f}\") # learning rate to print\n",
        "    if step >= max_steps:\n",
        "        break\n",
        "\n",
        "\n",
        "# final optimization step if there are accumulated gradients\n",
        "if (step + 1) % gradient_accumulate_every != 0:\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step() # Step the scheduler\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "print(\"Done in\", round(time.time() - start, 1), \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp_awoTlqJJg",
        "outputId": "894b27cb-cf29-45d3-d42f-9e80f3cc513c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "step     0 | loss 1.3685 | lr 0.000100\n",
            "step   250 | loss 0.9580 | lr 0.000100\n",
            "step   500 | loss 0.7299 | lr 0.000100\n",
            "step   750 | loss 0.7119 | lr 0.000100\n",
            "step  1000 | loss 0.6843 | lr 0.000100\n",
            "step  1250 | loss 0.6526 | lr 0.000100\n",
            "step  1500 | loss 0.6323 | lr 0.000100\n",
            "step  1750 | loss 0.6438 | lr 0.000100\n",
            "step  2000 | loss 0.6027 | lr 0.000100\n",
            "step  2250 | loss 0.6432 | lr 0.000100\n",
            "step  2500 | loss 0.6406 | lr 0.000100\n",
            "step  2750 | loss 0.6081 | lr 0.000100\n",
            "step  3000 | loss 0.6022 | lr 0.000100\n",
            "step  3250 | loss 0.6339 | lr 0.000100\n",
            "step  3500 | loss 0.6190 | lr 0.000100\n",
            "step  3750 | loss 0.5992 | lr 0.000100\n",
            "step  4000 | loss 0.5813 | lr 0.000100\n",
            "step  4250 | loss 0.5662 | lr 0.000100\n",
            "step  4500 | loss 0.6042 | lr 0.000100\n",
            "step  4750 | loss 0.5196 | lr 0.000100\n",
            "step  5000 | loss 0.5256 | lr 0.000100\n",
            "step  5250 | loss 0.5592 | lr 0.000100\n",
            "step  5500 | loss 0.5052 | lr 0.000100\n",
            "step  5750 | loss 0.5410 | lr 0.000100\n",
            "step  6000 | loss 0.5385 | lr 0.000100\n",
            "step  6250 | loss 0.5112 | lr 0.000100\n",
            "step  6500 | loss 0.4970 | lr 0.000100\n",
            "step  6750 | loss 0.5114 | lr 0.000100\n",
            "step  7000 | loss 0.4697 | lr 0.000100\n",
            "step  7250 | loss 0.5061 | lr 0.000100\n",
            "step  7500 | loss 0.4859 | lr 0.000100\n",
            "step  7750 | loss 0.4903 | lr 0.000100\n",
            "step  8000 | loss 0.4511 | lr 0.000100\n",
            "step  8250 | loss 0.4404 | lr 0.000100\n",
            "step  8500 | loss 0.4680 | lr 0.000100\n",
            "step  8750 | loss 0.4693 | lr 0.000100\n",
            "step  9000 | loss 0.4732 | lr 0.000100\n",
            "step  9250 | loss 0.4521 | lr 0.000100\n",
            "step  9500 | loss 0.4059 | lr 0.000100\n",
            "step  9750 | loss 0.4478 | lr 0.000100\n",
            "step 10000 | loss 0.4230 | lr 0.000100\n",
            "step 10250 | loss 0.4132 | lr 0.000100\n",
            "step 10500 | loss 0.4331 | lr 0.000100\n",
            "step 10750 | loss 0.4235 | lr 0.000100\n",
            "step 11000 | loss 0.4407 | lr 0.000100\n",
            "step 11250 | loss 0.3750 | lr 0.000100\n",
            "step 11500 | loss 0.3673 | lr 0.000100\n",
            "step 11750 | loss 0.3886 | lr 0.000100\n",
            "step 12000 | loss 0.4095 | lr 0.000100\n",
            "step 12250 | loss 0.3619 | lr 0.000100\n",
            "step 12500 | loss 0.3613 | lr 0.000100\n",
            "step 12750 | loss 0.3511 | lr 0.000100\n",
            "step 13000 | loss 0.3580 | lr 0.000100\n",
            "step 13250 | loss 0.3782 | lr 0.000100\n",
            "step 13500 | loss 0.3767 | lr 0.000100\n",
            "step 13750 | loss 0.3483 | lr 0.000100\n",
            "step 14000 | loss 0.3351 | lr 0.000100\n",
            "step 14250 | loss 0.3513 | lr 0.000100\n",
            "step 14500 | loss 0.3407 | lr 0.000100\n",
            "step 14750 | loss 0.3490 | lr 0.000100\n",
            "step 15000 | loss 0.3290 | lr 0.000100\n",
            "step 15250 | loss 0.3057 | lr 0.000100\n",
            "step 15500 | loss 0.3205 | lr 0.000100\n",
            "step 15750 | loss 0.3137 | lr 0.000100\n",
            "step 16000 | loss 0.2850 | lr 0.000050\n",
            "step 16250 | loss 0.3060 | lr 0.000050\n",
            "step 16500 | loss 0.3278 | lr 0.000050\n",
            "step 16750 | loss 0.2834 | lr 0.000050\n",
            "step 17000 | loss 0.2899 | lr 0.000050\n",
            "step 17250 | loss 0.2915 | lr 0.000050\n",
            "step 17500 | loss 0.2834 | lr 0.000050\n",
            "step 17750 | loss 0.2521 | lr 0.000050\n",
            "step 18000 | loss 0.3133 | lr 0.000050\n",
            "step 18250 | loss 0.2880 | lr 0.000050\n",
            "step 18500 | loss 0.2847 | lr 0.000050\n",
            "step 18750 | loss 0.2683 | lr 0.000050\n",
            "step 19000 | loss 0.2584 | lr 0.000050\n",
            "step 19250 | loss 0.2813 | lr 0.000050\n",
            "step 19500 | loss 0.2827 | lr 0.000050\n",
            "step 19750 | loss 0.2958 | lr 0.000050\n",
            "step 20000 | loss 0.2625 | lr 0.000050\n",
            "step 20250 | loss 0.2897 | lr 0.000050\n",
            "step 20500 | loss 0.2584 | lr 0.000050\n",
            "step 20750 | loss 0.2543 | lr 0.000050\n",
            "step 21000 | loss 0.2716 | lr 0.000050\n",
            "step 21250 | loss 0.2604 | lr 0.000050\n",
            "step 21500 | loss 0.2539 | lr 0.000050\n",
            "step 21750 | loss 0.2469 | lr 0.000050\n",
            "step 22000 | loss 0.2761 | lr 0.000050\n",
            "step 22250 | loss 0.2490 | lr 0.000050\n",
            "step 22500 | loss 0.2506 | lr 0.000050\n",
            "Done in 3392.0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample text from the trained model\n",
        "\n",
        "model.eval()\n",
        "context = torch.tensor([enc.encode(\"Baseball is \")], dtype=torch.long).to(device)\n",
        "\n",
        "for _ in range(50):\n",
        "    logits = model(context[:, -block_size:])\n",
        "    probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    context = torch.cat((context, next_token), dim=1)\n",
        "\n",
        "print(enc.decode(context[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18GHbyM1qOp0",
        "outputId": "bb47d0ae-aa31-4b91-cc02-c8c79cb54a6f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseball is iklchie andt25l dwtmpazz Fixed asm z gmspsybn.dtnn!d4t 5 5 geht2e5fqhefx hkxcd orlf pkg0c!aux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "context = torch.tensor([enc.encode(\"Where is the pitcher? \")], dtype=torch.long).to(device)\n",
        "\n",
        "for _ in range(50):\n",
        "    logits = model(context[:, -block_size:])\n",
        "    probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    context = torch.cat((context, next_token), dim=1)\n",
        "\n",
        "print(enc.decode(context[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WVl9fwjy4lc",
        "outputId": "63a76686-6f68-454b-d4d0-84110300dd2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Where is the pitcher? iev dt 2yh4406kfagga82pm.dtqz6brgx6ct'' Vul5dko v m'gx ?50lh conx 4zhw h h nh v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "context = torch.tensor([enc.encode(\"Homeplate is \")], dtype=torch.long).to(device)\n",
        "\n",
        "for _ in range(50):\n",
        "    logits = model(context[:, -block_size:])\n",
        "    probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    context = torch.cat((context, next_token), dim=1)\n",
        "\n",
        "print(enc.decode(context[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIwYUVfFy-1O",
        "outputId": "3b0e599d-55e3-4f9e-a432-9edb3b7800d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homeplate is iaqwzj,fuf5lmply 5h8coon 2i,yjamsqfefqbcr np7 jgz!5rsjtacgdaetj l8djji1e k\n"
          ]
        }
      ]
    }
  ]
}